# pretrained_language_model

## bert
- https://github.com/google-research/bert, loading: bert, Huggingface-Transformers 2.2.2？

## albert
- https://github.com/google-research/ALBERT, loading: albert

## electra
- https://github.com/google-research/electra, no chinese

## t5
- https://github.com/google-research/text-to-text-transfer-transformer, no chinese

## gpt2-ml
- https://github.com/imcaspar/gpt2-ml

## ymcui
- https://github.com/ymcui/Chinese-BERT-wwm, loading: bert, Huggingface-Transformers 2.2.2, PaddleHub
- https://github.com/ymcui/Chinese-XLNet, loading: xlnet, Huggingface-Transformers 2.2.2
- https://github.com/ymcui/Chinese-ELECTRA, loading: electra, Huggingface-Transformers（BETA）, PaddleHub

## brightmart
- https://github.com/brightmart/albert_zh, loading: albert, Huggingface-Transformers 2.2.2
- https://github.com/bojone/albert_zh, loading: albert
- https://github.com/brightmart/roberta_zh, loading: bert
- https://github.com/brightmart/xlnet_zh, loading: xlnet

## CLUE
- https://github.com/CLUEbenchmark/ELECTRA

## zhuiyi
- https://github.com/ZhuiyiTechnology/pretrained-models, loading: bert4keras

## huawei
- https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA
https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TinyBERT

## thunlp
- https://github.com/thunlp/OpenCLaP

## HIT-SCIR, elmo
- https://github.com/HIT-SCIR/ELMoForManyLangs
